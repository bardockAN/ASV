{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9920101,"sourceType":"datasetVersion","datasetId":6096635},{"sourceId":11565412,"sourceType":"datasetVersion","datasetId":7251391},{"sourceId":11821032,"sourceType":"datasetVersion","datasetId":7425275},{"sourceId":11843572,"sourceType":"datasetVersion","datasetId":7441285}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# SpeechBrain l√† m·ªôt th∆∞ vi·ªán m·∫°nh m·∫Ω v√† d·ªÖ d√πng cho c√°c t√°c v·ª• X·ª≠ l√Ω ti·∫øng n√≥i b·∫±ng AI\n# B·ªô c√¥ng c·ª• ph·∫ßn m·ªÅm ƒë√£ x√¢y d·ª±ng s·∫µn, gi√∫p b·∫°n l√†m vi·ªác hi·ªáu qu·∫£ h∆°n trong m·ªôt lƒ©nh v·ª±c c·ª• th·ªÉ nh∆∞:\n# AI, x·ª≠ l√Ω ti·∫øng n√≥i, th·ªã gi√°c m√°y t√≠nh, x·ª≠ l√Ω ng√¥n ng·ªØ...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:10:29.013017Z","iopub.execute_input":"2025-05-09T16:10:29.013740Z","iopub.status.idle":"2025-05-09T16:10:29.019033Z","shell.execute_reply.started":"2025-05-09T16:10:29.013697Z","shell.execute_reply":"2025-05-09T16:10:29.018008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√†i ƒë√∫ng phi√™n b·∫£n t∆∞∆°ng th√≠ch v·ªõi GPU Kaggle\n!pip install --no-cache-dir torch==2.5.1 torchaudio==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124\n# C√†i l·∫°i SpeechBrain\n!pip install --no-cache-dir speechbrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T02:51:50.486215Z","iopub.execute_input":"2025-05-12T02:51:50.486485Z","iopub.status.idle":"2025-05-12T02:52:59.210524Z","shell.execute_reply.started":"2025-05-12T02:51:50.486459Z","shell.execute_reply":"2025-05-12T02:52:59.209559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import speechbrain\nprint(speechbrain.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:44:02.175238Z","iopub.execute_input":"2025-05-10T13:44:02.175539Z","iopub.status.idle":"2025-05-10T13:44:06.742675Z","shell.execute_reply.started":"2025-05-10T13:44:02.175513Z","shell.execute_reply":"2025-05-10T13:44:06.741942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#SpeechBrain l√† toolkit ƒë√£ t√≠ch h·ª£p s·∫µn ECAPA‚ÄëTDNN v√† utilities fine‚Äëtune \n# os: l√†m vi·ªác v·ªõi h·ªá ƒëi·ªÅu h√†nh\n# torchaudio: x·ª≠ l√Ω √¢m th√†nh\n# pandas: x·ª≠ l√Ω b·∫£ng d·ªØ li·ªáu\n\nimport os, torchaudio, pandas as pd\nfrom pathlib import Path\nbase = Path(\"/kaggle/input/vietnam-celeb-dataset/full-dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:44:17.627411Z","iopub.execute_input":"2025-05-10T13:44:17.627701Z","iopub.status.idle":"2025-05-10T13:44:17.994252Z","shell.execute_reply.started":"2025-05-10T13:44:17.627679Z","shell.execute_reply":"2025-05-10T13:44:17.993515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (2)\n# M·ª•c ƒê√≠ch  Chuy·ªÉn ƒë·ªïi file (cleaned_utterances (1).txt) th√†nh m·ªôt file train_manifest.csv c√≥ 2 c·ªôt: wav, spk_id\n# L√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ d·∫°ng .txt (ch·ª©a danh s√°ch ƒë∆∞·ªùng d·∫´n .wav) th√†nh m·ªôt file \n#.csv c√≥ c·∫•u tr√∫c r√µ r√†ng (g·ªìm c·ªôt wav v√† spk_id), ƒë·ªÉ ph·ª•c v·ª• cho vi·ªác hu·∫•n luy·ªán m√¥ h√¨nh.\n# m·ªôt b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω r·∫•t quan tr·ªçng tr∆∞·ªõc khi fine-tune m√¥ h√¨nh ECAPA-TDNN: \n# t√°ch t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p validation t·ª´ file train_manifest.csv.\n\nfrom pathlib import Path\nimport pandas as pd\n\ntrain_txt = \"/kaggle/input/clean-utterances/cleaned_utterances (1).txt\"\nbase      = Path(\"/kaggle/input/vietnam-celeb-dataset/full-dataset\")\n\ntrain_rows = []\n\nwith open(train_txt) as f:\n    for line in f:\n        parts = line.strip().split()           # t√°ch b·∫±ng m·ªçi kho·∫£ng tr·∫Øng / tab\n        if len(parts) == 1:\n            # ƒë√£ d·∫°ng id00000/00000.wav\n            rel = parts[0]\n        elif len(parts) == 2:\n            # d·∫°ng \"id00000 00000.wav\" ho·∫∑c tab\n            rel = f\"{parts[0]}/{parts[1]}\"\n        else:\n            # d√≤ng l·∫° ‚Üí b·ªè qua\n            continue\n\n        wav_path = base / \"data\" / rel\n        if wav_path.exists():\n            spk = rel.split('/')[0]            # id00000\n            train_rows.append([str(wav_path), spk])\n\nprint(\"S·ªë file WAV h·ª£p l·ªá:\", len(train_rows))\n\npd.DataFrame(train_rows, columns=[\"wav\", \"spk_id\"]).to_csv(\n    \"train_manifest.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:44:21.933151Z","iopub.execute_input":"2025-05-10T13:44:21.933642Z","iopub.status.idle":"2025-05-10T13:46:59.658747Z","shell.execute_reply.started":"2025-05-10T13:44:21.933614Z","shell.execute_reply":"2025-05-10T13:46:59.658128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (3)\n# Tr·ªôn ng·∫´u nhi√™n v√† chia t·∫≠p train/validation\n# kh√¥ng ƒë∆∞·ª£c ƒë√°nh gi√° m√¥ h√¨nh tr√™n ch√≠nh t·∫≠p d·ªØ li·ªáu m√† b·∫°n ƒë√£ d√πng ƒë·ªÉ hu·∫•n luy·ªán.\n# N·∫øu l√†m v·∫≠y, m√¥ h√¨nh s·∫Ω ch·ªâ \"h·ªçc thu·ªôc\" (memorize) thay v√¨ h·ªçc kh√°i qu√°t (generalize).\nimport pandas as pd\n\n# 1) ƒê·ªçc train_manifest.csv\ndf = pd.read_csv(\"train_manifest.csv\")\n# M·ªôt DataFrame gi·ªëng nh∆∞ b·∫£ng d·ªØ li·ªáu d·∫°ng Excel, v·ªõi h√†ng v√† c·ªôt.\n\n# 2) Tr·ªôn ng·∫´u nhi√™n v√† chia\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\nn_train = int(len(df) * 0.9)\n\ntrain_df = df.iloc[:n_train]\nvalid_df = df.iloc[n_train:]\n\n# 3) Ghi hai file m·ªõi v√†o working dir\ntrain_df.to_csv(\"train_manifest.csv\", index=False) # d√πng ƒë·ªÉ train model\nvalid_df.to_csv(\"valid_manifest.csv\", index=False) # ki·ªÉm tra model khi train\n\nprint(f\"ƒê√£ t·∫°o: train_manifest.csv ({len(train_df)} samples)  |  valid_manifest.csv ({len(valid_df)} samples)\")\n# t·∫°o ƒë∆∞·ª£c 2 file .csv trong th∆∞ m·ª•c l√†m vi·ªác (working directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:47:14.275810Z","iopub.execute_input":"2025-05-10T13:47:14.276599Z","iopub.status.idle":"2025-05-10T13:47:14.521718Z","shell.execute_reply.started":"2025-05-10T13:47:14.276564Z","shell.execute_reply":"2025-05-10T13:47:14.521106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#(4)   CLEAN manifest + √©p ki·ªÉu spk_lbl + th√™m ID\nimport pandas as pd, os\n\ndef fix_manifest(csv_in, csv_out=None):\n    if csv_out is None:\n        csv_out = csv_in\n    df = pd.read_csv(csv_in)\n\n    # Gi·ªØ file WAV t·ªìn t·∫°i\n    df = df[df[\"wav\"].apply(os.path.exists)].reset_index(drop=True)\n\n    # N·∫øu thi·∫øu spk_lbl: t·∫°o t·ª´ spk_id\n    if \"spk_lbl\" not in df.columns:\n        spk2idx = {spk: i for i, spk in enumerate(sorted(df.spk_id.unique()))}\n        df[\"spk_lbl\"] = df.spk_id.map(spk2idx)\n\n    # üëâ Lu√¥n √©p v·ªÅ numeric, b·ªè h√†ng r·ªóng, √©p int\n    df[\"spk_lbl\"] = pd.to_numeric(df[\"spk_lbl\"], errors=\"coerce\")\n    df = df.dropna(subset=[\"spk_lbl\"]).reset_index(drop=True)\n    df[\"spk_lbl\"] = df[\"spk_lbl\"].astype(int)\n\n    # Th√™m ID n·∫øu thi·∫øu\n    if \"ID\" not in df.columns:\n        df.insert(0, \"ID\", range(len(df)))\n\n    df.to_csv(csv_out, index=False)\n    print(f\"{csv_out} ‚úÖ  {len(df)} rows\")\n\n# ch·∫°y cho c·∫£ train & valid\nfix_manifest(\"train_manifest.csv\")\nfix_manifest(\"valid_manifest.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:48:23.815994Z","iopub.execute_input":"2025-05-10T13:48:23.816605Z","iopub.status.idle":"2025-05-10T13:49:02.937931Z","shell.execute_reply.started":"2025-05-10T13:48:23.816581Z","shell.execute_reply":"2025-05-10T13:49:02.937292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (5)\n# ==== NEW CELL: l√†m s·∫°ch manifest & g·∫Øn nh√£n s·ªë ====\n# l·ªçc b·ªè ƒë∆∞·ªùng d·∫´n WAV kh√¥ng t·ªìn t·∫°i\n# map spk_id‚ÄØ(string) ‚Ü¶‚ÄØspk_lbl‚ÄØ(int) ƒë·ªÉ d√πng cross‚Äëentropy.\n\nimport pandas as pd, os\n\ndef load_clean(csv_path):\n    \"\"\"ƒê·ªçc CSV, b·ªè WAV thi·∫øu & strip kho·∫£ng tr·∫Øng spk_id.\"\"\"\n    df = pd.read_csv(csv_path)\n    df = df[df[\"wav\"].apply(os.path.exists)].reset_index(drop=True)\n    df[\"spk_id\"] = df[\"spk_id\"].astype(str).str.strip()\n    return df\n\ntrain_df = load_clean(\"train_manifest.csv\")\nvalid_df = load_clean(\"valid_manifest.csv\")\n\n# 1) map spk_id -> index (ch·ªâ d·ª±a tr√™n TRAIN)\nspk2idx = {spk: i for i, spk in enumerate(sorted(train_df.spk_id.unique()))}\ntrain_df[\"spk_lbl\"] = train_df.spk_id.map(spk2idx)\n\n# 2) Gi·ªØ VALID ch·ªâ g·ªìm speaker ƒë√£ c√≥ trong TRAIN\nvalid_df = valid_df[valid_df.spk_id.isin(spk2idx)].reset_index(drop=True)\nvalid_df[\"spk_lbl\"] = valid_df.spk_id.map(spk2idx)\n\n# 3) B·ªè m·ªçi h√†ng c√≤n NaN (ph√≤ng h·ªù) & √©p ki·ªÉu int\ntrain_df = train_df.dropna(subset=[\"spk_lbl\"]).copy()\nvalid_df = valid_df.dropna(subset=[\"spk_lbl\"]).copy()\ntrain_df[\"spk_lbl\"] = train_df[\"spk_lbl\"].astype(int)\nvalid_df[\"spk_lbl\"] = valid_df[\"spk_lbl\"].astype(int)\n\n# 4) L∆∞u l·∫°i\ntrain_df.to_csv(\"train_manifest.csv\", index=False)\nvalid_df.to_csv(\"valid_manifest.csv\", index=False)\n\nnum_spk = len(spk2idx)\nprint(f\"Manifest OK ‚úî  num_spk={num_spk} | \"\n      f\"train={len(train_df)} | valid={len(valid_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:49:11.831228Z","iopub.execute_input":"2025-05-10T13:49:11.831798Z","iopub.status.idle":"2025-05-10T13:49:48.468871Z","shell.execute_reply.started":"2025-05-10T13:49:11.831775Z","shell.execute_reply":"2025-05-10T13:49:48.468197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"assert not train_df[\"spk_lbl\"].isna().any()\nassert not valid_df[\"spk_lbl\"].isna().any()\nprint(\"Kh√¥ng c√≤n spk_lbl tr·ªëng ‚úÖ\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (6)\n# B∆∞·ªõc n·∫°p m√¥ h√¨nh ECAPA-TDNN ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫µn (pretrained)\n# v√† ƒë√≥ng bƒÉng ph·∫ßn \"kh·ªëi n√£o\" ch√≠nh (feature extractor) ƒë·ªÉ th·ª±c hi·ªán fine-tuning hi·ªáu qu·∫£ h∆°n.\n# N·∫°p ECAPA‚ÄëTDNN & ƒë√≥ng bƒÉng kh·ªëi n√£o\n# ==== N·∫°p ECAPA‚ÄëTDNN g·ªëc & t·∫°o classifier m·ªõi ====\n# N·∫°p ECAPA‚ÄëTDNN ƒë√£ hu·∫•n luy·ªán s·∫µn & g·∫Øn classifier cho 1‚ÄØ000 speaker Vi·ªát.\n# ‚Äë Ch·ªâ ƒë√≥ng bƒÉng 2 block ƒë·∫ßu (layer1, layer2) ‚Üí ph·∫ßn c√≤n l·∫°i v·∫´n fine‚Äëtune.\n# ‚Äë KH√îNG t·∫°o optimizer, KH√îNG t·∫°o checkpointer (s·∫Ω l√†m ·ªü Cell¬†8).\n\nfrom speechbrain.pretrained import SpeakerRecognition\nimport torch.nn as nn\nimport os\n\n# 1) Load model ECAPA‚ÄëTDNN (SpeechBrain)\npretrained = SpeakerRecognition.from_hparams(\n    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n    savedir=\"pretrained_ecapa\"            # cache t·∫°i /kaggle/working\n)\n\ncompute_features = pretrained.mods.compute_features\nmean_var_norm   = pretrained.mods.mean_var_norm\nembedding_model = pretrained.mods.embedding_model\n\n# 2) Freeze CH·ªà hai block ƒë·∫ßu ‚Üí gi·ªØ layer3/4, ASP ƒë∆∞·ª£c h·ªçc ti·∫øp\nfor name, module in embedding_model.named_children():\n    if name in [\"layer1\", \"layer2\"]:\n        for p in module.parameters():\n            p.requires_grad = False\n\n# 3) Classifier m·ªõi cho num_spk l·ªõp (num_spk ƒë√£ t√≠nh ·ªü Cell¬†5)\nclassifier = nn.Linear(192, num_spk, bias=True)\nnn.init.xavier_uniform_(classifier.weight)\nnn.init.zeros_(classifier.bias)\n\nprint(\"‚úÖ ECAPA loaded  ‚Ä¢  frozen layers: layer1¬†&¬†layer2  ‚Ä¢  new classifier ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:51:08.252400Z","iopub.execute_input":"2025-05-10T13:51:08.253134Z","iopub.status.idle":"2025-05-10T13:51:12.804390Z","shell.execute_reply.started":"2025-05-10T13:51:08.253090Z","shell.execute_reply":"2025-05-10T13:51:12.803639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- wav_collate m·ªõi ---\ndef wav_collate(batch):\n    sigs  = [item[\"sig\"] for item in batch]              # list [1,T_i]\n    lbls  = torch.tensor([item[\"lbl\"] for item in batch])\n    lens  = torch.tensor([s.shape[-1] for s in sigs], dtype=torch.float32)\n\n    max_len = lens.max().int().item()\n    padded  = torch.stack([F.pad(s, (0, max_len - s.shape[-1])) for s in sigs])\n\n    # tr·∫£ v·ªÅ T·ª∞ NHI√äN: lens (s·ªë frame), kh√¥ng chia max_len\n    return {\"sig\": (padded, lens), \"lbl\": lbls}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:52:51.821947Z","iopub.execute_input":"2025-05-10T13:52:51.822523Z","iopub.status.idle":"2025-05-10T13:52:51.827778Z","shell.execute_reply.started":"2025-05-10T13:52:51.822499Z","shell.execute_reply":"2025-05-10T13:52:51.826938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (7) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# ==== DATASET & DATALOADER (ƒë√£ fix) ====\nfrom speechbrain.dataio.dataset import DynamicItemDataset\nfrom speechbrain.dataio.dataloader import SaveableDataLoader\nimport torchaudio, torch\n\nMAX_SEC = 8              # gi·ªõi h·∫°n 8 gi√¢y\nMAX_SAMPLES = 16000 * MAX_SEC\n\ndef audio_pipeline(wav_path):\n    sig, _ = torchaudio.load(wav_path)\n\n    # g·ªôp v·ªÅ mono\n    if sig.dim() == 2:\n        sig = sig.mean(0, keepdim=True)\n\n    # ---- C·∫ÆT NG·∫™U NHI√äN n·∫øu >8s ----\n    if sig.shape[-1] > MAX_SAMPLES:\n        start = torch.randint(0, sig.shape[-1] - MAX_SAMPLES, (1,)).item()\n        sig = sig[:, start:start + MAX_SAMPLES]\n\n    return sig          # [1, T]\n\ndef label_pipeline(lbl):\n    return torch.tensor(int(lbl))\n\ndef make_ds(csv_path):\n    ds = DynamicItemDataset.from_csv(csv_path=csv_path)\n    ds.add_dynamic_item(audio_pipeline, takes=[\"wav\"],  provides=[\"sig\"])\n    ds.add_dynamic_item(label_pipeline, takes=[\"spk_lbl\"], provides=[\"lbl\"])\n    ds.set_output_keys([\"sig\", \"lbl\"])\n    return ds\n\ntrain_set = make_ds(\"train_manifest.csv\")\nvalid_set = make_ds(\"valid_manifest.csv\")\n\n# --------------------- gi·∫£m batch_size c√≤n 4 ---------------------\ntrain_dl = SaveableDataLoader(train_set, batch_size=4, shuffle=True,\n                              collate_fn=wav_collate)\nvalid_dl = SaveableDataLoader(valid_set, batch_size=4,\n                              collate_fn=wav_collate)\n\nprint(\"Dataloader OK  ‚Äì  train batches:\", len(train_dl),\n      \"| valid:\", len(valid_dl))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:53:01.363857Z","iopub.execute_input":"2025-05-10T13:53:01.364103Z","iopub.status.idle":"2025-05-10T13:53:01.628312Z","shell.execute_reply.started":"2025-05-10T13:53:01.364087Z","shell.execute_reply":"2025-05-10T13:53:01.627704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (8) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# ƒê·ªãnh nghƒ©a Fine-tune Brain + hu·∫•n luy·ªán 20 epoch\n# (ƒë√£ s·ª≠a l·ªói ‚Äústagefrom ‚Ä¶‚Äù v√† th√™m logic unfreeze layer3/4 sau epoch 2)\n\n# ---------- Imports v√† thi·∫øt l·∫≠p th∆∞ m·ª•c checkpoints ----------\nfrom speechbrain import Brain, Stage\nfrom speechbrain.utils.metric_stats import MetricStats\nfrom speechbrain.utils.checkpoints import Checkpointer\nimport torch, torch.nn.functional as F\nimport os\n\nckpt_dir = \"checkpoints\"\nos.makedirs(ckpt_dir, exist_ok=True)\n\nclass FineTuneBrain(Brain):\n    def on_fit_start(self):\n        super().on_fit_start()\n        # T·∫°o 2 metric stats ri√™ng cho train v√† valid\n        self.train_acc_metric = MetricStats(metric=lambda p, t: (p.argmax(-1) == t).float())\n        self.valid_acc_metric = MetricStats(metric=lambda p, t: (p.argmax(-1) == t).float())\n        self.unfrozen = False\n\n    def on_stage_start(self, stage, epoch):\n        # Logic unfreeze sau epoch 2\n        if stage == Stage.TRAIN and epoch == 2 and not self.unfrozen:\n            for name, module in embedding_model.named_children():\n                if name not in [\"layer1\", \"layer2\"]:\n                    for p in module.parameters():\n                        p.requires_grad = True\n            existing = {id(p) for g in self.optimizer.param_groups for p in g[\"params\"]}\n            new_params = [p for p in embedding_model.parameters() if p.requires_grad and id(p) not in existing]\n            if new_params:\n                self.optimizer.add_param_group({\"params\": new_params, \"lr\": 1e-4})\n            self.unfrozen = True\n            print(f\">> Unfroze layer3+4+ASP ‚Äî added {len(new_params)} new params\")\n\n    def compute_forward(self, batch, stage=Stage.TRAIN):\n        wavs, lens = batch[\"sig\"]\n        wavs = wavs.squeeze(1).to(self.device)\n        lens = lens.to(self.device)\n        feats  = mean_var_norm(compute_features(wavs), lens)\n        embeds = embedding_model(feats, lens)\n        if embeds.dim() == 3:\n            embeds = embeds.mean(dim=1)\n        embeds = F.normalize(embeds, p=2, dim=-1)\n        logits = classifier(embeds)\n        return logits\n\n    def compute_objectives(self, predictions, batch, stage):\n        labels = batch[\"lbl\"].to(self.device)\n        loss   = F.cross_entropy(predictions, labels)\n        batch_size = labels.size(0)\n        dummy_ids  = [0] * batch_size\n\n        # Ghi nh·∫≠n train acc khi stage=TRAIN, valid acc khi stage!=TRAIN\n        if stage == Stage.TRAIN:\n            self.train_acc_metric.append(dummy_ids, predictions, labels)\n        else:\n            self.valid_acc_metric.append(dummy_ids, predictions, labels)\n\n        return loss\n\n    def fit_batch(self, batch):\n        loss = super().fit_batch(batch)\n        # gradient accumulation: b∆∞·ªõc optimizer sau m·ªói 4 mini‚Äêbatch\n        if (self.step + 1) % 4 == 0:\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n        return loss\n\n    def on_stage_end(self, stage, stage_loss, epoch):\n        if stage == Stage.TRAIN:\n            train_acc = self.train_acc_metric.summarize(\"average\") * 100\n            lr = self.optimizer.param_groups[0][\"lr\"]\n            print(f\"Epoch {epoch} ‚Ä¢ train_acc={train_acc:.2f}% ‚Ä¢ train_loss={stage_loss:.3f} ‚Ä¢ lr={lr:.2e}\")\n            self.train_acc_metric.clear()\n\n        elif stage == Stage.VALID:\n            valid_acc = self.valid_acc_metric.summarize(\"average\") * 100\n            print(f\"Epoch {epoch} ‚Ä¢ valid_acc={valid_acc:.2f}% ‚Ä¢ valid_loss={stage_loss:.3f}\")\n            self.valid_acc_metric.clear()\n            self.checkpointer.save_checkpoint()\n\n# ---------- Kh·ªüi t·∫°o v√† g·ªçi fit() ----------\nbrain = FineTuneBrain(\n    modules = {\n        \"compute_features\":   compute_features,\n        \"mean_var_norm\":      mean_var_norm,\n        \"embedding_model\":    embedding_model,\n        \"classifier\":         classifier,\n    },\n    opt_class    = torch.optim.AdamW,\n    hparams      = {\"lr\": 3e-4, \"weight_decay\": 1e-4},\n    run_opts     = {\"device\": \"cuda\", \"dtype\": \"float16\"},\n    checkpointer = Checkpointer(ckpt_dir),\n)\n\nbrain.fit(\n    range(60),\n    train_set=train_dl,\n    valid_set=valid_dl\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:48:10.389560Z","iopub.execute_input":"2025-05-10T15:48:10.390134Z","iopub.status.idle":"2025-05-10T16:16:03.880314Z","shell.execute_reply.started":"2025-05-10T15:48:10.390111Z","shell.execute_reply":"2025-05-10T16:16:03.879352Z"}},"outputs":[],"execution_count":null}]}